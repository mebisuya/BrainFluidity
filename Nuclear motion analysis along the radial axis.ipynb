{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c45861-1794-4976-9512-072ce5447901",
   "metadata": {},
   "source": [
    "This notebook reproduces the **Nuclear tracking and motion analysis along the radial axis** described in the manuscript.  \n",
    "\n",
    "## Analysis Workflow\n",
    "- **Preprocessing**  \n",
    "  - Sample drift is corrected with MultiStackReg.\n",
    "  - Nuclear radial trajectories are extracted in Fiji using the Manual Track plugin.  \n",
    "  - CSV files are imported, headers cleaned, and tracks sorted by time.  \n",
    "  - Derived columns include:\n",
    "    - `time` (frame index × interval in minutes)\n",
    "    - `index` (track identifier)\n",
    "    - Interval displacements (relative to the previous frame)\n",
    "    - Instantaneous speeds  \n",
    "\n",
    "- **Filtering**  \n",
    "  - **Radial fluctuations**: keep only trajectories ≥150 min; truncate to the first 150 min.  \n",
    "  - **Basal migrations**: i.e., **migration from apical surface**; keep only trajectories ≥40 min; truncate to the first 40 min.  \n",
    "\n",
    "- **Quantifications**  \n",
    "  - **Mean speed**: calculated as the total path length (sum of interval displacements) divided by total duration.  \n",
    "  - **Net displacement**: displacement vectors relative to the initial position (t=0),projected onto the radial axis.\n",
    "\n",
    "- **Variability Analysis**  \n",
    "  - Net displacement traces are fit with linear regression.  \n",
    "  - Root-mean-square error (RMSE) of residuals is computed to quantify fluctuation variability for both radial fluctuations and basal migrations.  \n",
    "\n",
    "- **Outputs**  \n",
    "  - Filtered and truncated trajectory tables (Excel).  \n",
    "  - Net displacement, mean speed, instantaneous speed\n",
    "  - Net displacement traces with regression overlays.  \n",
    "  - RMSE values per nucleus (Excel summary + plots).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2c91c-0002-4146-9f57-a16a58fd30bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5ddad-7d9a-4cab-824d-9cb7f6d1f122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241e93f-e9bc-4426-8bb1-10d341544c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.cm import get_cmap\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576725cf-4d62-491b-a4d4-5aa2d7b6e95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c44fd50c-4db3-4971-a8f5-64df52842b2d",
   "metadata": {},
   "source": [
    "# Section1: Below is to calculate the net displacement from the initial position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30701a0d-dc4f-4ad7-b6f1-53d646dd214f",
   "metadata": {},
   "source": [
    "!!!!! NOTE that initial position refers to apical point, net displacement refers to the 'DistanceToApical' in the script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64383587-2dd0-4d92-9e34-86aeb8287e89",
   "metadata": {},
   "source": [
    "NOTE that the blocks under Section1 applies to both nuclear radial fluctuations and basal migrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c378e-6e21-424a-a206-3b65f59d6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === read data \n",
    "file_path = \"input_folder_name\" ## data structure: file_path {condition} >> cvs. file \n",
    "os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e070d-5434-4f77-b271-6ff0dfc96757",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = 0.5303 # image resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f998d6-c25e-4719-9046-930e587c014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(file_path):\n",
    "    file_extension = os.path.splitext(f)[-1]\n",
    "    file_name = os.path.splitext(f)[0]\n",
    "    if file_extension == '.csv':\n",
    "        # Read the CSV data\n",
    "        data = pd.read_csv(file_path + f, encoding='utf-8', encoding_errors=\"ignore\")\n",
    "        \n",
    "        # Rename columns to avoid special characters\n",
    "        data.columns = [\"Track n\", \"Slice\", \"X\", \"Y\", \"Distance\", \"Velocity\", \"Pixel_Value\"]\n",
    "        df = data[[\"Track n\", \"X\", \"Y\", \"Distance\", \"Velocity\"]]\n",
    "\n",
    "        # Group by 'Track n'\n",
    "        grouped_data = df.groupby('Track n')\n",
    "        \n",
    "        # Create a new column named 'DistanceToApical'\n",
    "        df['DistanceToApical'] = np.nan\n",
    "        h_list = []\n",
    "        \n",
    "        # Calculate distance from the apical point \n",
    "        for name, group in grouped_data:\n",
    "            first_row = group.iloc[0]\n",
    "            distances = np.sqrt((group['X'] - first_row['X'])**2 + (group['Y'] - first_row['Y'])**2)\n",
    "            distances = round(distances * pixel, 2)\n",
    "            \n",
    "            df.loc[df['Track n'] == name, 'DistanceToApical'] = distances.tolist()\n",
    "\n",
    "        # Remove the first three rows in each group: first 2 points are just to label the apical and basal surface\n",
    "        df = df.groupby('Track n').apply(lambda group: group.iloc[3:]).reset_index(drop=True)\n",
    "        df['Time'] = df.groupby('Track n').cumcount() * 10\n",
    "        \n",
    "        # Save \n",
    "        grouped_data_df['DistanceToApical'].to_excel(f\"{file_path}/{file_name}_distance_to_apical.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffdaac-b126-4ab4-bfb4-d80ab52fc8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4569b11-978b-4ec9-817b-52b26e325c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e197ce-9bcf-4166-916d-f1695692807c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20e9e4af-bb3d-42ab-9bc1-3d1c4dc28217",
   "metadata": {},
   "source": [
    "# Section2:Below is for nuclear **radial fluctuations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e2d3a-036f-46d7-b992-3f06cdd511c3",
   "metadata": {},
   "source": [
    "### Step 1: read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f8cee-5659-43b0-b283-80b352dd9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"input-folder-name\"\n",
    "output_folder_path = 'output-folder-name'\n",
    "N_POINTS = 16   # read only the first 16 time points (i.e., 150-min trajectory)\n",
    "\n",
    "DEFAULT_DT_MIN = 10.0  \n",
    "\n",
    "OUT_SUMMARY_XLSX = \"metrics_summary.xlsx\"\n",
    "OUT_SPEED_TS_XLSX = \"instantaneous_speeds.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b15403-8465-47c3-b437-fa57e06b03cc",
   "metadata": {},
   "source": [
    "### Step 2: data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7f7e5-3454-4bc7-bf7b-982bfa0b044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coerce_numeric(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def infer_dt_minutes(time_col):\n",
    "    \"\"\"Infer dt (in minutes) from the median spacing of time stamps.\"\"\"\n",
    "    t = coerce_numeric(time_col).dropna().to_numpy()\n",
    "    if t.size < 2:\n",
    "        return DEFAULT_DT_MIN\n",
    "    diffs = np.diff(np.sort(t))\n",
    "    dt = np.median(diffs) if diffs.size else DEFAULT_DT_MIN\n",
    "    if not np.isfinite(dt) or dt <= 0:\n",
    "        return DEFAULT_DT_MIN\n",
    "    return float(dt)\n",
    "\n",
    "def make_index_final(idx_val, track_val):\n",
    "    return f\"Index={idx_val}|Track={track_val}\"\n",
    "\n",
    "def process_group_firstN(g, file_name, sheet_name, idx_val, track_val, N=N_POINTS):\n",
    "    \"\"\"\n",
    "    Clean group, enforce first N time points, compute metrics.\n",
    "    Returns (summary_dict, timeseries_df) or (None, None) if < N rows.\n",
    "    \"\"\"\n",
    "\n",
    "    df = g.copy()\n",
    "    if \"Time\" not in df or \"Distance\" not in df:\n",
    "        return None, None\n",
    "\n",
    "    df[\"Time\"] = coerce_numeric(df[\"Time\"])\n",
    "    df[\"Distance\"] = coerce_numeric(df[\"Distance\"])  # signed Δx per interval (µm)\n",
    "    df = df.dropna(subset=[\"Time\", \"Distance\"]).sort_values(\"Time\")\n",
    "\n",
    "    if len(df) < N:\n",
    "        return None, None\n",
    "\n",
    "    # Keep only the first N time points\n",
    "    dfN = df.head(N).copy()\n",
    "    index_final = make_index_final(idx_val, track_val)\n",
    "    dfN[\"index-final\"] = index_final\n",
    "\n",
    "    # Infer dt using the selected N rows\n",
    "    dt_min = infer_dt_minutes(dfN[\"Time\"])\n",
    "\n",
    "    # Instantaneous speed (µm/min): |Δx| / dt\n",
    "    delta_x = dfN[\"Distance\"].to_numpy()\n",
    "    v = np.abs(delta_x) / dt_min\n",
    "\n",
    "    # Path length & mean speed over these N intervals\n",
    "    path_length_um = float(np.sum(np.abs(delta_x)))\n",
    "    total_time_min = float(len(dfN) * dt_min)\n",
    "    mean_speed = path_length_um / total_time_min if total_time_min > 0 else np.nan\n",
    "\n",
    "\n",
    "    # Summary row (per track)\n",
    "    summary = dict(\n",
    "        file=file_name,\n",
    "        sheet=sheet_name,\n",
    "        Index=idx_val,\n",
    "        Track_n=track_val,\n",
    "        index_final=index_final,\n",
    "        n_timepoints_used=n,\n",
    "        dt_minutes=dt_min,\n",
    "        total_time_minutes=total_time_min,\n",
    "        path_length_um=path_length_um,\n",
    "        mean_speed_um_per_min=mean_speed,\n",
    "     \n",
    "    )\n",
    "\n",
    "    # Per-interval time series (use [t_start, t_end] for clarity)\n",
    "    t_end = dfN[\"Time\"].to_numpy()\n",
    "    t_start = t_end - dt_min\n",
    "\n",
    "    ts = pd.DataFrame({\n",
    "        \"file\": file_name,\n",
    "        \"sheet\": sheet_name,\n",
    "        \"Index\": idx_val,\n",
    "        \"Track_n\": track_val,\n",
    "        \"index-final\": index_final,\n",
    "        \"t_start_min\": t_start,\n",
    "        \"t_end_min\": t_end,\n",
    "        \"delta_x_um\": delta_x,\n",
    "        \"inst_speed_um_per_min\": v,\n",
    "    })\n",
    "\n",
    "    return summary, ts\n",
    "\n",
    "def run_pipeline():\n",
    "    all_summaries = []\n",
    "    all_ts = []\n",
    "\n",
    "    xlsx_paths = glob.glob(os.path.join(BASE_FOLDER, \"*.xlsx\"))\n",
    "    if not xlsx_paths:\n",
    "        print(f\"[warn] No .xlsx files found in {BASE_FOLDER}\")\n",
    "        return\n",
    "\n",
    "    for filepath in xlsx_paths:\n",
    "        file_name = Path(filepath).name\n",
    "        try:\n",
    "            xls = pd.ExcelFile(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] Could not open {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            try:\n",
    "                df = xls.parse(sheet_name)\n",
    "            except Exception as e:\n",
    "                print(f\"[skip] {file_name} / {sheet_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Check required columns\n",
    "            required = {\"Index\", \"Track n\", \"Time\", \"Distance\"}\n",
    "            missing = required - set(df.columns)\n",
    "            if missing:\n",
    "                print(f\"[skip] {file_name} / {sheet_name}: missing columns {missing}\")\n",
    "                continue\n",
    "\n",
    "            # Group by droplet (Index × Track n)\n",
    "            groups = df.groupby([\"Index\", \"Track n\"], dropna=False)\n",
    "            for (idx_val, track_val), g in groups:\n",
    "                summary, ts = process_group_firstN(\n",
    "                    g, file_name, sheet_name, idx_val, track_val, N=N_POINTS\n",
    "                )\n",
    "                if summary is None:\n",
    "                    continue\n",
    "                all_summaries.append(summary)\n",
    "                all_ts.append(ts)\n",
    "\n",
    "    # Collate and write Excel outputs\n",
    "    if all_summaries:\n",
    "        summary_df = pd.DataFrame(all_summaries).sort_values(\n",
    "            [\"file\", \"sheet\", \"Index\", \"Track_n\"]\n",
    "        )\n",
    "        out_path = os.path.join(BASE_FOLDER, OUT_SUMMARY_XLSX)\n",
    "        with pd.ExcelWriter(out_path, engine=\"xlsxwriter\") as writer:\n",
    "            summary_df.to_excel(writer, index=False, sheet_name=\"summary\")\n",
    "        print(f\"[ok] wrote {OUT_SUMMARY_XLSX} with {len(summary_df)} tracks\")\n",
    "    else:\n",
    "        print(\"[warn] No qualifying tracks (>= 15 time points). No summary written.\")\n",
    "\n",
    "    if all_ts:\n",
    "        ts_df = pd.concat(all_ts, ignore_index=True).sort_values(\n",
    "            [\"file\", \"sheet\", \"Index\", \"Track_n\", \"t_end_min\"]\n",
    "        )\n",
    "        out_path = os.path.join(BASE_FOLDER, OUT_SPEED_TS_XLSX)\n",
    "        with pd.ExcelWriter(out_path, engine=\"xlsxwriter\") as writer:\n",
    "            ts_df.to_excel(writer, index=False, sheet_name=\"instantaneous\")\n",
    "        print(f\"[ok] wrote {OUT_SPEED_TS_XLSX} with {len(ts_df)} intervals\")\n",
    "    else:\n",
    "        print(\"[warn] No timeseries written (no groups with >= 15 points).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b5549-acb3-468b-96e0-7beae950fd04",
   "metadata": {},
   "source": [
    "### Step 3: visualize instantaneous speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b48b7-cd26-4dbf-93de-b5c9ae4e698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== LOAD DATA ====\n",
    "INSTANT_XLSX = OUT_SPEED_TS_XLSX   # excel generated from the cell above\n",
    "SHEET_NAME = \"instantaneous\"  \n",
    "xlsx_path = os.path.join(BASE_FOLDER, INSTANT_XLSX)\n",
    "df = pd.read_excel(xlsx_path, sheet_name=SHEET_NAME)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6ed80-b9a5-485a-ba30-9778067c547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIG ====\n",
    "POINT_SIZE = 25\n",
    "LINE_WIDTH = 1\n",
    "ALPHA_LINE = 0.9\n",
    "ALPHA_SCAT = 0.75\n",
    "CMAP_NAME = \"tab20\"                         \n",
    "LEGEND_OUTSIDE = True                       \n",
    "GROUP_BY_SHEET = False       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35389cf1-1fe6-4d96-9d7f-8633be0d6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure numeric & clean\n",
    "for col in [\"t_end_min\", \"inst_speed_um_per_min\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"file\", \"index-final\", \"t_end_min\", \"inst_speed_um_per_min\"])\n",
    "\n",
    "# helper: nice title\n",
    "def pretty_title(file_name, sheet_name=None):\n",
    "    title = f\"Instantaneous speed (line + scatter)\\n{file_name}\"\n",
    "    if sheet_name:\n",
    "        title += f\"  |  sheet: {sheet_name}\"\n",
    "    return title\n",
    "\n",
    "# helper: color iterator per plot\n",
    "def color_cycle(n, cmap_name=CMAP_NAME):\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    return [cmap(i % cmap.N) for i in range(n)]\n",
    "\n",
    "# ==== PLOTTING ====\n",
    "out_dir = BASE_FOLDER\n",
    "files = df[\"file\"].unique()\n",
    "\n",
    "for file_name in files:\n",
    "    df_file = df[df[\"file\"] == file_name]\n",
    "\n",
    "    sheet_groups = [(\"\", df_file)] if not GROUP_BY_SHEET else df_file.groupby(\"sheet\", dropna=False)\n",
    "\n",
    "    for sheet_name, df_subset in sheet_groups:\n",
    "        tracks = list(df_subset[\"index-final\"].unique())\n",
    "        if len(tracks) == 0:\n",
    "            continue\n",
    "\n",
    "        colors = color_cycle(len(tracks))\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(5, 4))\n",
    "\n",
    "        for color, idx_final in zip(colors, tracks):\n",
    "            g = df_subset[df_subset[\"index-final\"] == idx_final].copy()\n",
    "            g = g.sort_values(\"t_end_min\")\n",
    "\n",
    "            x = g[\"t_end_min\"].to_numpy()\n",
    "            y = g[\"inst_speed_um_per_min\"].to_numpy()\n",
    "\n",
    "            plt.plot(\n",
    "                x, y, color=color, lw=LINE_WIDTH, alpha=ALPHA_LINE\n",
    "            )\n",
    "            plt.scatter(\n",
    "                x, y, color=color, s=POINT_SIZE, alpha=ALPHA_SCAT, edgecolors=\"none\"\n",
    "            )\n",
    "\n",
    "        plt.xlabel(\"Time (min)\")\n",
    "        plt.ylabel(\"Instantaneous speed (µm/min)\")\n",
    "        ttl = pretty_title(file_name, sheet_name if GROUP_BY_SHEET else None)\n",
    "        plt.title(ttl)\n",
    "\n",
    "\n",
    "        xmin, xmax = np.nanmin(df_subset[\"t_end_min\"]), np.nanmax(df_subset[\"t_end_min\"])\n",
    "        if np.isfinite(xmin) and np.isfinite(xmax) and xmin != xmax:\n",
    "            pad = 0.02 * (xmax - xmin)\n",
    "            plt.xlim(xmin - pad, xmax + pad)\n",
    "\n",
    "        stem = Path(file_name).stem\n",
    "        if GROUP_BY_SHEET:\n",
    "            sheet_label = str(sheet_name).replace(\"/\", \"_\")\n",
    "            out_pdf = os.path.join(out_dir, f\"inst_speed_{stem}__{sheet_label}.pdf\")\n",
    "        else:\n",
    "            out_pdf = os.path.join(out_dir, f\"inst_speed_{stem}.pdf\")\n",
    "\n",
    "        plt.savefig(out_pdf, dpi=300)\n",
    "        # plt.close()\n",
    "\n",
    "        print(f\"[ok] saved {out_pdf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d31174-2aa8-4358-9c3c-fa45475c0aeb",
   "metadata": {},
   "source": [
    "### Step 4: fit the net displacement with linear regression and compute the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6485a22-bb4b-4fb8-8146-28c440994184",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_SUMMARY_XLSX = \"rmse_regression-radial-fluctuations-summary.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c6a8b-dfd3-4c6a-bf66-5e27ac7d7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== HELPERS ====\n",
    "def coerce_numeric(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def make_index_final(idx_val, track_val):\n",
    "    return f\"Index={idx_val}|Track={track_val}\"\n",
    "\n",
    "def process_group_rmse_regression(g, file_name, sheet_name, idx_val, track_val, N=N_POINTS):\n",
    "    \"\"\"\n",
    "    For one droplet track:\n",
    "    - Require at least N points\n",
    "    - Keep only first N time points\n",
    "    - Fit linear regression\n",
    "    - Compute RMSE of residuals\n",
    "    \"\"\"\n",
    "    if not {\"Time\", \"Normalized_DistanceToApical\"} <= set(g.columns):\n",
    "        return None\n",
    "\n",
    "    df = g.copy()\n",
    "    df[\"Time\"] = coerce_numeric(df[\"Time\"])\n",
    "    df[\"Normalized_DistanceToApical\"] = coerce_numeric(df[\"Normalized_DistanceToApical\"])\n",
    "    df = df.dropna(subset=[\"Time\", \"Normalized_DistanceToApical\"]).sort_values(\"Time\")\n",
    "\n",
    "    if len(df) < N:\n",
    "        return None  # skip if not enough points\n",
    "\n",
    "    # restrict to first N time points\n",
    "    dfN = df.head(N).copy()\n",
    "\n",
    "    # regression input\n",
    "    X = dfN[\"Time\"].to_numpy().reshape(-1, 1)\n",
    "    y = dfN[\"Normalized_DistanceToApical\"].to_numpy()\n",
    "\n",
    "    # fit regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # residuals & RMSE\n",
    "    residuals = y - y_pred\n",
    "    rmse = float(np.sqrt(np.mean(residuals**2)))\n",
    "\n",
    "    return dict(\n",
    "        file=file_name,\n",
    "        sheet=sheet_name,\n",
    "        Index=idx_val,\n",
    "        Track_n=track_val,\n",
    "        index_final=make_index_final(idx_val, track_val),\n",
    "        n_points_used=len(dfN),\n",
    "        slope=model.coef_[0],\n",
    "        intercept=model.intercept_,\n",
    "        rmse_regression=rmse\n",
    "    )\n",
    "\n",
    "# ==== MAIN ====\n",
    "def run_pipeline():\n",
    "    results = []\n",
    "\n",
    "    xlsx_paths = glob.glob(os.path.join(BASE_FOLDER, \"*.xlsx\"))\n",
    "    if not xlsx_paths:\n",
    "        print(f\"[warn] No .xlsx files found in {BASE_FOLDER}\")\n",
    "        return\n",
    "\n",
    "    for filepath in xlsx_paths:\n",
    "        file_name = Path(filepath).name\n",
    "        try:\n",
    "            xls = pd.ExcelFile(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] Could not open {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            try:\n",
    "                df = xls.parse(sheet_name)\n",
    "            except Exception as e:\n",
    "                print(f\"[skip] {file_name} / {sheet_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            required = {\"Index\", \"Track n\", \"Time\", \"Normalized_DistanceToApical\"}\n",
    "            missing = required - set(df.columns)\n",
    "            if missing:\n",
    "                print(f\"[skip] {file_name} / {sheet_name}: missing {missing}\")\n",
    "                continue\n",
    "\n",
    "            # group by droplet track\n",
    "            groups = df.groupby([\"Index\", \"Track n\"], dropna=False)\n",
    "            for (idx_val, track_val), g in groups:\n",
    "                summary = process_group_rmse_regression(\n",
    "                    g, file_name, sheet_name, idx_val, track_val, N=N_POINTS\n",
    "                )\n",
    "                if summary is not None:\n",
    "                    results.append(summary)\n",
    "\n",
    "    # write output\n",
    "    if results:\n",
    "        summary_df = pd.DataFrame(results).sort_values(\n",
    "            [\"file\", \"sheet\", \"Index\", \"Track_n\"]\n",
    "        )\n",
    "        out_path = os.path.join(BASE_FOLDER, OUT_SUMMARY_XLSX)\n",
    "        summary_df.to_excel(out_path, index=False, sheet_name=\"rmse_regression\")\n",
    "        print(f\"[ok] wrote {OUT_SUMMARY_XLSX} with {len(summary_df)} tracks\")\n",
    "    else:\n",
    "        print(\"[warn] No groups processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb217e6-02ac-4fef-9ab4-1c6d91ed5942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d0bfa-5153-4855-8a6a-1cc41b8ae263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be296f9-e01e-4f88-b5c0-b25605861725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f178e09a-ae68-4c09-9af6-495ffc57bbe4",
   "metadata": {},
   "source": [
    "# Section3:Below is for nuclear **basal migrations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1e5db-644c-404e-8c6d-0ec17deced84",
   "metadata": {},
   "source": [
    "### Step 1: read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a88cef-646b-4f54-b6ef-96a0500b3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"input-folder-name\"\n",
    "output_folder_path = 'output-folder-name'\n",
    "N_POINTS = 5   # read only the first 5 time points (i.e., 40-min trajectory)\n",
    "\n",
    "DEFAULT_DT_MIN = 10.0  \n",
    "\n",
    "OUT_SUMMARY_XLSX = \"metrics_summary.xlsx\"\n",
    "OUT_SPEED_TS_XLSX = \"instantaneous_speeds.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e888e51-b541-4947-aae5-6dd632e33e2d",
   "metadata": {},
   "source": [
    "### Step 2: data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7c02c-4512-4d54-9819-e3f3a2562d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coerce_numeric(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def infer_dt_minutes(time_col):\n",
    "    \"\"\"Infer dt (in minutes) from the median spacing of time stamps.\"\"\"\n",
    "    t = coerce_numeric(time_col).dropna().to_numpy()\n",
    "    if t.size < 2:\n",
    "        return DEFAULT_DT_MIN\n",
    "    diffs = np.diff(np.sort(t))\n",
    "    dt = np.median(diffs) if diffs.size else DEFAULT_DT_MIN\n",
    "    if not np.isfinite(dt) or dt <= 0:\n",
    "        return DEFAULT_DT_MIN\n",
    "    return float(dt)\n",
    "\n",
    "def make_index_final(idx_val, track_val):\n",
    "    return f\"Index={idx_val}|Track={track_val}\"\n",
    "\n",
    "def process_group_firstN(g, file_name, sheet_name, idx_val, track_val, N=N_POINTS):\n",
    "    \"\"\"\n",
    "    Clean group, enforce first N time points, compute metrics.\n",
    "    Returns (summary_dict, timeseries_df) or (None, None) if < N rows.\n",
    "    \"\"\"\n",
    "\n",
    "    df = g.copy()\n",
    "    if \"Time\" not in df or \"Distance\" not in df:\n",
    "        return None, None\n",
    "\n",
    "    df[\"Time\"] = coerce_numeric(df[\"Time\"])\n",
    "    df[\"Distance\"] = coerce_numeric(df[\"Distance\"])  # signed Δx per interval (µm)\n",
    "    df = df.dropna(subset=[\"Time\", \"Distance\"]).sort_values(\"Time\")\n",
    "\n",
    "    if len(df) < N:\n",
    "        return None, None\n",
    "\n",
    "    # Keep only the first N time points\n",
    "    dfN = df.head(N).copy()\n",
    "    index_final = make_index_final(idx_val, track_val)\n",
    "    dfN[\"index-final\"] = index_final\n",
    "\n",
    "    # Infer dt using the selected N rows\n",
    "    dt_min = infer_dt_minutes(dfN[\"Time\"])\n",
    "\n",
    "    # Instantaneous speed (µm/min): |Δx| / dt\n",
    "    delta_x = dfN[\"Distance\"].to_numpy()\n",
    "    v = np.abs(delta_x) / dt_min\n",
    "\n",
    "    # Path length & mean speed over these N intervals\n",
    "    path_length_um = float(np.sum(np.abs(delta_x)))\n",
    "    total_time_min = float(len(dfN) * dt_min)\n",
    "    mean_speed = path_length_um / total_time_min if total_time_min > 0 else np.nan\n",
    "\n",
    "\n",
    "    # Summary row (per track)\n",
    "    summary = dict(\n",
    "        file=file_name,\n",
    "        sheet=sheet_name,\n",
    "        Index=idx_val,\n",
    "        Track_n=track_val,\n",
    "        index_final=index_final,\n",
    "        n_timepoints_used=n,\n",
    "        dt_minutes=dt_min,\n",
    "        total_time_minutes=total_time_min,\n",
    "        path_length_um=path_length_um,\n",
    "        mean_speed_um_per_min=mean_speed,\n",
    "     \n",
    "    )\n",
    "\n",
    "    # Per-interval time series (use [t_start, t_end] for clarity)\n",
    "    t_end = dfN[\"Time\"].to_numpy()\n",
    "    t_start = t_end - dt_min\n",
    "\n",
    "    ts = pd.DataFrame({\n",
    "        \"file\": file_name,\n",
    "        \"sheet\": sheet_name,\n",
    "        \"Index\": idx_val,\n",
    "        \"Track_n\": track_val,\n",
    "        \"index-final\": index_final,\n",
    "        \"t_start_min\": t_start,\n",
    "        \"t_end_min\": t_end,\n",
    "        \"delta_x_um\": delta_x,\n",
    "        \"inst_speed_um_per_min\": v,\n",
    "    })\n",
    "\n",
    "    return summary, ts\n",
    "\n",
    "def run_pipeline():\n",
    "    all_summaries = []\n",
    "    all_ts = []\n",
    "\n",
    "    xlsx_paths = glob.glob(os.path.join(BASE_FOLDER, \"*.xlsx\"))\n",
    "    if not xlsx_paths:\n",
    "        print(f\"[warn] No .xlsx files found in {BASE_FOLDER}\")\n",
    "        return\n",
    "\n",
    "    for filepath in xlsx_paths:\n",
    "        file_name = Path(filepath).name\n",
    "        try:\n",
    "            xls = pd.ExcelFile(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] Could not open {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            try:\n",
    "                df = xls.parse(sheet_name)\n",
    "            except Exception as e:\n",
    "                print(f\"[skip] {file_name} / {sheet_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Check required columns\n",
    "            required = {\"Index\", \"Track n\", \"Time\", \"Distance\"}\n",
    "            missing = required - set(df.columns)\n",
    "            if missing:\n",
    "                print(f\"[skip] {file_name} / {sheet_name}: missing columns {missing}\")\n",
    "                continue\n",
    "\n",
    "            # Group by droplet (Index × Track n)\n",
    "            groups = df.groupby([\"Index\", \"Track n\"], dropna=False)\n",
    "            for (idx_val, track_val), g in groups:\n",
    "                summary, ts = process_group_firstN(\n",
    "                    g, file_name, sheet_name, idx_val, track_val, N=N_POINTS\n",
    "                )\n",
    "                if summary is None:\n",
    "                    continue\n",
    "                all_summaries.append(summary)\n",
    "                all_ts.append(ts)\n",
    "\n",
    "    # Collate and write Excel outputs\n",
    "    if all_summaries:\n",
    "        summary_df = pd.DataFrame(all_summaries).sort_values(\n",
    "            [\"file\", \"sheet\", \"Index\", \"Track_n\"]\n",
    "        )\n",
    "        out_path = os.path.join(BASE_FOLDER, OUT_SUMMARY_XLSX)\n",
    "        with pd.ExcelWriter(out_path, engine=\"xlsxwriter\") as writer:\n",
    "            summary_df.to_excel(writer, index=False, sheet_name=\"summary\")\n",
    "        print(f\"[ok] wrote {OUT_SUMMARY_XLSX} with {len(summary_df)} tracks\")\n",
    "    else:\n",
    "        print(\"[warn] No qualifying tracks (>= 5 time points). No summary written.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34463dd9-8514-4114-9c23-2ae8ce46c65d",
   "metadata": {},
   "source": [
    "### Step 3: fit the net displacement with linear regression and compute the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16378091-ddab-4165-a1ed-4cba59541908",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_SUMMARY_XLSX = \"rmse_regression-basal-fluctuation-summary.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c284ad9-053a-422a-b3f0-4a8848ba5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== HELPERS ====\n",
    "def coerce_numeric(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def process_group_rmse_regression(g, file_name, sheet_name, idx_val, N=N_POINTS):\n",
    "    \"\"\"\n",
    "    For one droplet track (grouped by 'index1'):\n",
    "      - Make Normalized_DistanceToApical = DistanceToApical - DistanceToApical[0]\n",
    "      - Use normalised_time as x\n",
    "      - Require at least N points; keep only first N by time\n",
    "      - Fit y = a*x + b (numpy.polyfit)\n",
    "      - RMSE = sqrt(mean((y - yhat)^2))\n",
    "    Returns a summary dict or None.\n",
    "    \"\"\"\n",
    "    required_cols = {\"normalised_time\", \"DistanceToApical\"}\n",
    "    if not required_cols.issubset(g.columns):\n",
    "        return None\n",
    "\n",
    "    df = g.copy()\n",
    "    df[\"normalised_time\"] = coerce_numeric(df[\"normalised_time\"])\n",
    "    df[\"DistanceToApical\"] = coerce_numeric(df[\"DistanceToApical\"])\n",
    "    df = df.dropna(subset=[\"normalised_time\", \"DistanceToApical\"]).sort_values(\"normalised_time\")\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    # build Normalized_DistanceToApical (relative to the first observed value in time)\n",
    "    first_val = df[\"DistanceToApical\"].iloc[0]\n",
    "    df[\"Normalized_DistanceToApical\"] = df[\"DistanceToApical\"] - first_val\n",
    "\n",
    "    # require at least N points\n",
    "    if len(df) < N:\n",
    "        return None\n",
    "\n",
    "    # keep first N by time\n",
    "    dfN = df.head(N).copy()\n",
    "\n",
    "    # regression inputs\n",
    "    x = dfN[\"normalised_time\"].to_numpy()\n",
    "    y = dfN[\"Normalized_DistanceToApical\"].to_numpy()\n",
    "\n",
    "    # fit y ~ a*x + b using numpy (no sklearn dependency)\n",
    "    # polyfit returns [slope, intercept] for deg=1\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    y_hat = slope * x + intercept\n",
    "\n",
    "    # RMSE of residuals around the fitted line\n",
    "    residuals = y - y_hat\n",
    "    rmse = float(np.sqrt(np.mean(residuals**2)))\n",
    "\n",
    "    return dict(\n",
    "        file=file_name,\n",
    "        sheet=sheet_name,\n",
    "        index_final=idx_val,                 \n",
    "        n_points_used=len(dfN),            \n",
    "        slope=slope,\n",
    "        intercept=intercept,\n",
    "        rmse_regression=rmse\n",
    "    )\n",
    "\n",
    "# ==== MAIN ====\n",
    "def run_pipeline():\n",
    "    results = []\n",
    "\n",
    "    xlsx_paths = glob.glob(os.path.join(BASE_FOLDER, \"*.xlsx\"))\n",
    "    if not xlsx_paths:\n",
    "        print(f\"[warn] No .xlsx files found in {BASE_FOLDER}\")\n",
    "        return\n",
    "\n",
    "    for filepath in xlsx_paths:\n",
    "        file_name = Path(filepath).name\n",
    "        try:\n",
    "            xls = pd.ExcelFile(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] Could not open {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            try:\n",
    "                df = xls.parse(sheet_name)\n",
    "            except Exception as e:\n",
    "                print(f\"[skip] {file_name} / {sheet_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            required = {\"index1\", \"normalised_time\", \"DistanceToApical\"}\n",
    "            missing = required - set(df.columns)\n",
    "            if missing:\n",
    "                print(f\"[skip] {file_name} / {sheet_name}: missing {missing}\")\n",
    "                continue\n",
    "\n",
    "            # group by droplet track (index1)\n",
    "            for idx_val, g in df.groupby(\"index1\", dropna=False):\n",
    "                summary = process_group_rmse_regression(\n",
    "                    g, file_name, sheet_name, idx_val, N=N_POINTS\n",
    "                )\n",
    "                if summary is not None:\n",
    "                    results.append(summary)\n",
    "\n",
    "    # write output\n",
    "    if results:\n",
    "        summary_df = pd.DataFrame(results).sort_values([\"file\", \"sheet\", \"index_final\"])\n",
    "        out_path = os.path.join(BASE_FOLDER, OUT_SUMMARY_XLSX)\n",
    "        summary_df.to_excel(out_path, index=False, sheet_name=\"rmse_regression\")\n",
    "        print(f\"[ok] wrote {OUT_SUMMARY_XLSX} with {len(summary_df)} tracks\")\n",
    "    else:\n",
    "        print(\"[warn] No groups processed (check columns / N_POINTS threshold).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab6c4b-3027-4a97-b089-8e9d7873c522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
